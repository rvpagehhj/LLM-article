# LLM-article

### :star2: Recommendations :star2:
- **Transformer:** Attention Is All You Need. [[paper](https://arxiv.org/abs/1706.03762)]
- **Bert:** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. [[paper](https://arxiv.org/abs/1810.04805)]
- 
